{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9MOFMil8pDh4",
        "y7KeMGyLVECM",
        "hw2pfrklwCJm",
        "BBYjM8G2wI-X",
        "FKiXwWiMw1MP"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Region-Based Segmentation"
      ],
      "metadata": {
        "id": "9MOFMil8pDh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# access google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "Lb-RVGgtpUbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load zip file from drive\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Replace 'path/to/your/zipfile.zip' with the actual path to your zip file in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/MSFD.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/') # Extracts to the current working directory (/content/)\n"
      ],
      "metadata": {
        "id": "a4tj2Ynh0_RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Otsu thresholding"
      ],
      "metadata": {
        "id": "jNIp50vrpI6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "def calculate_IoU(original_image, thresholded_image):\n",
        "    # Compute IoU\n",
        "    # Reshape to the shape fo the thresholded_image\n",
        "    original_image = cv2.resize(original_image, (thresholded_image.shape[1], thresholded_image.shape[0]))\n",
        "    intersection = np.logical_and(original_image, thresholded_image)\n",
        "    union = np.logical_or(original_image, thresholded_image)\n",
        "    iou_score = np.sum(intersection) / np.sum(union)\n",
        "    return iou_score\n",
        "\n",
        "def calculate_Dice(original_image, thresholded_image):\n",
        "    # calculate dice score\n",
        "    # Reshape to the shape fo the thresholded_image\n",
        "    original_image = cv2.resize(original_image, (thresholded_image.shape[1], thresholded_image.shape[0]))\n",
        "    intersection = np.logical_and(original_image, thresholded_image)\n",
        "    dice_score = 2.0 * np.sum(intersection) / (np.sum(original_image) + np.sum(thresholded_image))\n",
        "    return dice_score\n",
        "\n",
        "train_folder = '/content/MSFD/1/face_crop'\n",
        "test_folder = '/content/MSFD/1/face_crop_segmentation'\n",
        "\n",
        "train_images = os.listdir(train_folder)\n",
        "test_images = os.listdir(test_folder)\n",
        "\n",
        "# Create sets for quick lookup\n",
        "test_image_set = set(test_images)\n",
        "\n",
        "total_iou = 0\n",
        "total_dice = 0\n",
        "image_count = 0\n",
        "unmatched_count = 0\n",
        "\n",
        "for train_filename in train_images:\n",
        "    if train_filename in test_image_set:\n",
        "        image_path = os.path.join(train_folder, train_filename)\n",
        "        gt_path = os.path.join(test_folder, train_filename)\n",
        "\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        gt = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if image is not None and gt is not None:\n",
        "            # Apply Otsu's thresholding\n",
        "            _, thresholded_image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "            iou = calculate_IoU(gt, thresholded_image)\n",
        "            total_iou += iou\n",
        "            dice = calculate_Dice(gt, thresholded_image)\n",
        "            total_dice += dice\n",
        "            image_count += 1\n",
        "    else:\n",
        "        unmatched_count += 1\n",
        "\n",
        "if image_count > 0:\n",
        "    average_iou = total_iou / image_count\n",
        "    average_dice = total_dice / image_count\n",
        "    print(\"IoU for Otsu thresholding:\", average_iou)\n",
        "    print(\"Dice for Otsu thresholding:\", average_dice)\n",
        "    print(\"Unmatched images:\", unmatched_count)\n",
        "else:\n",
        "    print(\"No matching images found in the specified folders.\")\n"
      ],
      "metadata": {
        "id": "4x-yB6IQ8Fw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainable"
      ],
      "metadata": {
        "id": "y7KeMGyLVECM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "def mse(mask1, mask2):\n",
        "    \"\"\"Compute Mean Squared Error\"\"\"\n",
        "    return np.mean((mask1.astype(\"float\") - mask2.astype(\"float\")) ** 2)\n",
        "\n",
        "def optimize_thresholds(image_paths, gt_paths):\n",
        "    best_mse = float('inf')\n",
        "    best_thresholds = None\n",
        "\n",
        "    # Try different HSV threshold values\n",
        "    for h_low in range(0, 180, 10):\n",
        "        for h_high in range(h_low+10, 180, 10):\n",
        "            for s_low in range(50, 255, 20):\n",
        "                for s_high in range(s_low+20, 255, 20):\n",
        "                    for v_low in range(50, 255, 20):\n",
        "                        for v_high in range(v_low+20, 255, 20):\n",
        "\n",
        "                            total_mse = 0\n",
        "                            count = 0\n",
        "\n",
        "                            for img_path, gt_path in zip(image_paths, gt_paths):\n",
        "                                img = cv2.imread(img_path)\n",
        "                                gt = cv2.imread(gt_path, 0)  # Load GT mask in grayscale\n",
        "                                gt = cv2.threshold(gt, 127, 255, cv2.THRESH_BINARY)[1]  # Binarize\n",
        "\n",
        "                                # Convert to HSV and apply threshold\n",
        "                                hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "                                lower = np.array([h_low, s_low, v_low])\n",
        "                                upper = np.array([h_high, s_high, v_high])\n",
        "                                mask = cv2.inRange(hsv, lower, upper)\n",
        "\n",
        "                                # Compute MSE\n",
        "                                total_mse += mse(mask, gt)\n",
        "                                count += 1\n",
        "\n",
        "                            avg_mse = total_mse / count\n",
        "\n",
        "                            if avg_mse < best_mse:\n",
        "                                best_mse = avg_mse\n",
        "                                best_thresholds = (h_low, h_high, s_low, s_high, v_low, v_high)\n",
        "\n",
        "    return best_thresholds, best_mse\n",
        "\n",
        "# Load dataset\n",
        "path_to_images = '/content/MSFD/1/face_crop'\n",
        "path_to_gt = '/content/MSFD/1/face_crop_segmentation'\n",
        "image_paths = sorted(glob.glob(path_to_images + \"/*.jpg\"))  # Modify with actual path\n",
        "gt_paths = sorted(glob.glob(path_to_gt + \"/*.jpg\"))  # Modify with actual path\n",
        "\n",
        "best_thresholds, best_mse = optimize_thresholds(image_paths, gt_paths)\n",
        "\n",
        "print(\"Best Thresholds:\", best_thresholds)\n",
        "print(\"Lowest MSE:\", best_mse)\n"
      ],
      "metadata": {
        "id": "e0_8x9viVJY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "JS9WiDUZ0Ybl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chandrikadeb7/Face-Mask-Detection.git"
      ],
      "metadata": {
        "id": "rJH2vbrB1VJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "x7gWXEtj0c1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Classifier"
      ],
      "metadata": {
        "id": "TtOayTTN4siL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (128, 128, 3)"
      ],
      "metadata": {
        "id": "MGP-ddHawbHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "EmGUBo-KwSHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers, models, Sequential\n",
        "from keras.regularizers import l2\n",
        "\n",
        "model = Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "1mtpyRT5wVcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load a Model"
      ],
      "metadata": {
        "id": "sszQ58mb6lVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: load a model\n",
        "from keras import models\n",
        "\n",
        "model = models.load_model(\"/content/mask_detection_model_small.h5\")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "j3oUXxVA6oLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "18zyjAcHwNGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam, SGD\n",
        "from keras.losses import BinaryCrossentropy\n",
        "\n",
        "optimizers = {\n",
        "    \"adam\": Adam(learning_rate=0.001),\n",
        "    \"sgd\": SGD(learning_rate=0.001, momentum=0.9)\n",
        "}\n",
        "\n",
        "losses = {\n",
        "    \"bce\": BinaryCrossentropy(label_smoothing=0.10),\n",
        "    \"scce\": \"sparse_categorical_crossentropy\"\n",
        "}\n",
        "model.compile(optimizer=optimizers[\"adam\"], loss=losses[\"bce\"], metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "m7Tf-fNNVfJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing:\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define ImageDataGenerator for real-time augmentation and memory efficiency\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Normalize pixel values\n",
        "\n",
        "batch_size = 32\n",
        "# Load train and validation sets using flow_from_directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'Face-Mask-Detection/dataset/',\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'  # Training set\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    'Face-Mask-Detection/dataset/',\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'  # Validation set\n",
        ")\n"
      ],
      "metadata": {
        "id": "29W7mMLsMDhy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Get class labels from generator\n",
        "labels = train_generator.classes  # Get all training labels\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "print(\"Class Weights:\", class_weights)\n",
        "\n",
        "val_labels = val_generator.classes  # Get all validation labels\n",
        "val_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(val_labels), y=val_labels)\n",
        "val_class_weights_dict = dict(enumerate(val_class_weights))\n",
        "\n",
        "print(\"Validation Class Weights:\", val_class_weights)"
      ],
      "metadata": {
        "id": "x6QxX7IEPkXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,  # Keep patience at 10\n",
        "    min_delta=0.001,  # Ignore very small improvements\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    class_weight=class_weights_dict,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "jLtFFlBF5-0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Get prediction probabilities\n",
        "y_pred_probs = model.predict(val_generator)\n",
        "y_true = val_generator.classes\n",
        "\n",
        "# Handle cases where model outputs two probabilities (for binary classification)\n",
        "if y_pred_probs.shape[1] == 2:\n",
        "    y_pred_probs = y_pred_probs[:, 1]  # Use probability of the positive class\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_probs)\n",
        "\n",
        "# Compute F1 scores\n",
        "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)  # Avoid division by zero\n",
        "\n",
        "# Find best threshold (maximize F1-score)\n",
        "best_threshold = thresholds[np.argmax(f1_scores)]\n",
        "\n",
        "print(f\"Best Threshold: {best_threshold:.4f}\")\n",
        "\n",
        "# Apply threshold for classification\n",
        "y_pred_new = (y_pred_probs > best_threshold).astype(int)\n"
      ],
      "metadata": {
        "id": "d385jj2fXYet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(y_pred_probs, bins=50, edgecolor=\"black\")\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Predicted Probabilities\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ui5pUnhRXlvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions (probabilities)\n",
        "y_pred = model.predict(val_generator)\n",
        "\n",
        "# Handle cases where model outputs two probabilities (for binary classification)\n",
        "if y_pred.shape[1] == 2:\n",
        "    y_pred = y_pred[:, 1]  # Take probability of the positive class\n",
        "\n",
        "# Convert probabilities to class labels using a default threshold of 0.5\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Get true labels\n",
        "y_true = val_generator.classes\n",
        "\n",
        "# Print Classification Report\n",
        "print(classification_report(y_true, y_pred_classes, target_names=['No Mask', 'Mask']))\n"
      ],
      "metadata": {
        "id": "mSD_RQVvUinp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dWBw2b0IS_At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save('mask_detection_model_small.h5')"
      ],
      "metadata": {
        "id": "7D7veavdgx_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating on entire dataset"
      ],
      "metadata": {
        "id": "_95RF5j1iMUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "from tensorflow.keras.models import load_model\n",
        "# model = load_model('mask_detection_model_first.h5')\n",
        "\n",
        "# evaluate on entire dataset\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'Face-Mask-Detection/dataset/',\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_generator)\n",
        "print(f\"Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "id": "DZ8ZK_yGiQtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generated"
      ],
      "metadata": {
        "id": "eRlO8TN1dFSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Data Paths\n",
        "data_dir = \"/content/Face-Mask-Detection/dataset\"  # UPDATE this to your dataset folder\n",
        "\n",
        "# Image Data Generator with Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # 80% train, 20% val\n",
        ")\n",
        "\n",
        "# Load Data\n",
        "img_size = (128, 128)\n",
        "batch_size = 32\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Improved CNN Model\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(128, 128, 3)),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(32, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2,2),\n",
        "\n",
        "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(64, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2,2),\n",
        "\n",
        "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(128, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2,2),\n",
        "\n",
        "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv2D(256, (3,3), activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D(2,2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,  # Increased for better feature learning\n",
        "    callbacks=[early_stopping, lr_reduction]\n",
        ")\n",
        "\n",
        "# Evaluate Model\n",
        "y_pred_probs = model.predict(val_generator)\n",
        "y_true = val_generator.classes\n",
        "y_pred_classes = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "print(classification_report(y_true, y_pred_classes, target_names=['No Mask', 'Mask']))\n",
        "\n",
        "# Precision-Recall Curve\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_probs)\n",
        "plt.plot(recalls, precisions, label=\"Precision-Recall Curve\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Je16ZKjPdHPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qN9-o2HodHnJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}